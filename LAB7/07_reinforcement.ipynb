{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8370326d",
   "metadata": {},
   "source": [
    "# Machine Learning LAB 7: REINFORCEMENT LEARNING\n",
    "\n",
    "Course 2025/26: *F. Chiariotti (notebook by P. Talli)*\n",
    "\n",
    "The notebook contains some simple tasks about **REINFORCEMENT LEARNING**.\n",
    "\n",
    "Complete all the **required code sections**.\n",
    "\n",
    "### IMPORTANT for the exam:\n",
    "\n",
    "The functions you might be required to implement in the exam will have the same signature and parameters as the ones in the labs\n",
    "\n",
    "### Content\n",
    "\n",
    "In this lab we will implement and test two fundamental reinforcement learning algorithms: Q-Learning and SARSA. We will use a simple cliff environment to evaluate the performance of both agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc9f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "651075a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for the Environment\n",
    "class SimpleCliffEnv:\n",
    "    def __init__(self):\n",
    "        \"\"\" Tile layout (36=start, 47=goal, 37-46=cliff)\n",
    "        0\t1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t11\n",
    "        12\t13\t14\t15\t16\t17\t18\t19\t20\t21\t22\t23\n",
    "        24\t25\t26\t27\t28\t29\t30\t31\t32\t33\t34\t35\n",
    "        36\t37\t38\t39\t40\t41\t42\t43\t44\t45\t46\t47\n",
    "        \"\"\"\n",
    "        self.nS = 48  # number of states\n",
    "        self.nA = 4   # number of actions\n",
    "        self.P = np.zeros((self.nS, self.nA, self.nS)) # transition probabilities\n",
    "        self.R = np.zeros((self.nS, self.nA, self.nS)) # rewards\n",
    "\n",
    "        # Define transitions and rewards\n",
    "        for s in range(self.nS):\n",
    "            for a in range(self.nA):\n",
    "                if s == 47:  # goal state\n",
    "                    self.P[s, a, s] = 1.0\n",
    "                    self.R[s, a, s] = 0.0\n",
    "                else:\n",
    "                    next_s = self._get_next_state(s, a)\n",
    "                    self.P[s, a, next_s] = 1.0\n",
    "                    if next_s >= 37 and next_s <= 46:  # cliff states\n",
    "                        self.R[s, a, next_s] = -100.0\n",
    "                    else:\n",
    "                        self.R[s, a, next_s] = -1.0\n",
    "\n",
    "    def _get_next_state(self, s, a):\n",
    "        row, col = divmod(s, 12) #putts s in a matrix of values using modulo 12 (number of columns) division\n",
    "        if a == 0:  # action up\n",
    "            row = max(row - 1, 0)\n",
    "        elif a == 1:  # action right\n",
    "            col = min(col + 1, 11)\n",
    "        elif a == 2:  # action down\n",
    "            row = min(row + 1, 3)\n",
    "        elif a == 3:  # action left\n",
    "            col = max(col - 1, 0)\n",
    "        return row * 12 + col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03cb160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some utility functions\n",
    "def print_policy(policy):\n",
    "    action_symbols = ['↑', '→', '↓', '←']\n",
    "    for i in range(4):\n",
    "        row = ''\n",
    "        for j in range(12):\n",
    "            s = i * 12 + j\n",
    "            if s == 47:\n",
    "                row += ' G  '\n",
    "            elif s >= 37 and s <= 46:\n",
    "                row += ' C  '\n",
    "            else:\n",
    "                row += f' {action_symbols[policy[s]]}  '\n",
    "        print(row)\n",
    "\n",
    "def print_value_function(V):\n",
    "    # Visualize the value function as a heatmap\n",
    "    V_grid = V.reshape((4, 12))\n",
    "    # Cliff states set to NaN for better visualization\n",
    "    for s in range(37, 47):\n",
    "        row, col = divmod(s, 12)\n",
    "        V_grid[row, col] = np.nan\n",
    "    plt.figure(figsize=(5,2.5))\n",
    "    plt.imshow(V_grid, interpolation='nearest')\n",
    "    # Without axes \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    # Colorbar horizontal, set lower bound to -100\n",
    "    plt.colorbar(orientation='horizontal')\n",
    "    plt.title('Value Function Heatmap')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "692a0a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ↑   →   →   ←   →   ↓   ↓   ←   ↑   ↓   ↓   ←  \n",
      " ↓   ↑   ←   →   →   →   ↓   →   →   →   ←   ←  \n",
      " ↑   ↑   ↑   ←   ↓   ↓   ↓   ↑   ↑   ↓   ←   →  \n",
      " →   C   C   C   C   C   C   C   C   C   C   G  \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x250 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "policy = np.random.choice(4, size=48)  # Random policy for demonstration\n",
    "V = np.random.rand(48) * -10  # Random value function for demonstration\n",
    "print_policy(policy)\n",
    "print_value_function(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2df85e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Policy Evaluation function\n",
    "def policy_evaluation(env, policy, theta=1e-6, discount_factor=0.99):\n",
    "    \"\"\"This function takes a policy and an environment and returns the\n",
    "    value function for that policy.\n",
    "    Args:\n",
    "        env: The environment to evaluate the policy on.\n",
    "        policy: The policy to evaluate. [nS x nA] shaped matrix\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0 # Variable that tracks the maximum change in the value function\n",
    "        for s in range(env.nS):\n",
    "            ### TODO: implement the policy evaluation function\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f952fd3a",
   "metadata": {},
   "source": [
    "Remember what a value function represents\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\mid s_0 = s \\right]$$\n",
    "\n",
    "It is the expected cumulative reward starting from state $s$ and following policy $\\pi$.\n",
    "It can be computed iteratively using the Bellman expectation equation:\n",
    "$$V^\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s', r} p(s'| s, a) \\left[ r(s, a, s') + \\gamma V^\\pi(s') \\right]$$\n",
    "\n",
    "With ```policy_evaluation``` function we can evaluate stochastic policies where the action taken in each state is not deterministic.\n",
    "\n",
    "$$\\pi(a|s) = Prob(A_t = a | S_t = s)$$\n",
    "\n",
    "Let's test a uniform random policy in the cliff environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f0bd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.ones((48, 4))  # Uniform random policy\n",
    "policy = policy / policy.sum(axis=1, keepdims=True)  # Normalize to get probabilities\n",
    "V = policy_evaluation(SimpleCliffEnv(), policy)\n",
    "print_value_function(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ba2423",
   "metadata": {},
   "source": [
    "We can notice that the value function is very low in states close to the cliff, as the agent is likely to fall off and incur a large negative reward. \n",
    "The value function increases as we move away from the cliff, indicating that the agent is more likely to reach the goal without falling off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88622f87",
   "metadata": {},
   "source": [
    "### Q-Learning Agent\n",
    "Now let's implement and test a Q-Learning agent in the cliff environment. Q-Learning is an off-policy algorithm that learns the optimal action-value function, which can be used to derive the optimal policy.\n",
    "\n",
    "Q value function:\n",
    "$$Q^*(s, a) = \\mathbb{E} \\left[ r_t + \\gamma \\max_{a'} Q^*(s_{t+1}, a') \\mid s_t = s, a_t = a \\right]$$\n",
    "\n",
    "Update rule:\n",
    "$$Q(s, a) \\gets Q(s, a) + \\alpha \\left( r_t + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s, a) \\right)$$\n",
    "\n",
    "To explore the action space we use an epsilon-greedy policy, which selects a random action with probability epsilon, and the action that maximizes the Q-value with probability 1-epsilon.\n",
    "\n",
    "$$ \\pi(a|s) = \\begin{cases} \\frac{\\epsilon}{|A(s)|} & \\text{if } a \\neq \\arg\\max_{a'} Q(s, a') \\\\ 1 - \\epsilon + \\frac{\\epsilon}{|A(s)|} & \\text{if } a = \\arg\\max_{a'} Q(s, a') \\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5877cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q Learning Implementation\n",
    "# With epsilon-greedy action selection\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, learning_rate=0.1, discount_factor=0.99, exploration_prob=0.1):\n",
    "        self.env = env\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = exploration_prob\n",
    "        self.Q = np.zeros((env.nS, env.nA))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        ### TODO Implement an epsilon-greedy policy (the function should return a valid action index)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        ### TODO Update the Q-value table following the Q-learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00404d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_greedy_policy(Q):\n",
    "    policy = np.zeros(Q.shape[0], dtype=int)\n",
    "    for s in range(Q.shape[0]):\n",
    "        policy[s] = np.argmax(Q[s])\n",
    "    return policy\n",
    "\n",
    "def get_max_q_values(Q):\n",
    "    V = np.max(Q, axis=1)\n",
    "    return V\n",
    "\n",
    "def training_loop(env, agent, num_episodes=500):\n",
    "    for episode in range(num_episodes):\n",
    "        if episode % 200 == 0:\n",
    "            policy = get_greedy_policy(agent.Q)\n",
    "            print_policy(policy)\n",
    "            V = get_max_q_values(agent.Q)\n",
    "            print_value_function(V)\n",
    "        state = 36  # Start state\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state_probs = env.P[state, action]\n",
    "            next_state = np.random.choice(np.arange(env.nS), p=next_state_probs)\n",
    "            reward = env.R[state, action, next_state]\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            if state == 47:  # Goal state\n",
    "                done = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2ae22d",
   "metadata": {},
   "source": [
    "Test the Q-Learning agent in the cliff environment for 1000 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edfba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_agent = QLearningAgent(SimpleCliffEnv(), exploration_prob=0.1)\n",
    "num_episodes = 1000\n",
    "\n",
    "training_loop(SimpleCliffEnv(), q_agent, num_episodes)\n",
    "policy = get_greedy_policy(q_agent.Q)\n",
    "print_policy(policy)\n",
    "V = get_max_q_values(q_agent.Q)\n",
    "print_value_function(V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e13d7b",
   "metadata": {},
   "source": [
    "# SARSA Agent\n",
    "Now let's implement and test a SARSA agent in the cliff environment. SARSA is an on-policy algorithm that learns the action-value function for the policy being followed.\n",
    "SARSA value function:\n",
    "$$Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ r_t + \\gamma Q^\\pi(s_{t+1}, a_{t+1}) \\mid s_t = s, a_t = a \\right]$$\n",
    "\n",
    "Update rule:\n",
    "$$Q(s, a) \\gets Q(s, a) + \\alpha \\left( r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s, a) \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fe6059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA Implementation\n",
    "class SARSAAgent:\n",
    "    def __init__(self, env, learning_rate=0.1, discount_factor=0.99, exploration_prob=0.1):\n",
    "        self.env = env\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = exploration_prob\n",
    "        self.Q = np.zeros((env.nS, env.nA))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        ### TODO Implement an epsilon-greedy policy (the function should return a valid action index)\n",
    "\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, next_action):\n",
    "        ### TODO Update the Q-value table following the SARSA algorithm (remember the difference with Q-learning!)\n",
    "\n",
    "def sarsa_training_loop(env, agent, num_episodes=500):\n",
    "    for episode in range(num_episodes):\n",
    "        state = 36  # Start state\n",
    "        action = agent.choose_action(state)\n",
    "        done = False\n",
    "        while not done:\n",
    "            next_state_probs = env.P[state, action]\n",
    "            next_state = np.random.choice(np.arange(env.nS), p=next_state_probs)\n",
    "            reward = env.R[state, action, next_state]\n",
    "            next_action = agent.choose_action(next_state)\n",
    "            agent.learn(state, action, reward, next_state, next_action)\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            if state == 47:  # Goal state\n",
    "                done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4551ee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SARSA Agent\n",
    "sarsa_agent = SARSAAgent(SimpleCliffEnv(), exploration_prob=0.1)\n",
    "sarsa_training_loop(SimpleCliffEnv(), sarsa_agent, num_episodes=1000)\n",
    "policy = get_greedy_policy(sarsa_agent.Q)\n",
    "print_policy(policy)\n",
    "V = get_max_q_values(sarsa_agent.Q)\n",
    "print_value_function(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b35fce",
   "metadata": {},
   "source": [
    "We can notice that the value function learned by the SARSA agent is generally lower than that of the Q-Learning agent, especially in states close to the cliff. This is because SARSA takes into account the action taken in the next state, which may lead to falling off the cliff, while Q-Learning always assumes the best possible action will be taken. As a result, SARSA tends to be more conservative in its value estimates, leading to a safer policy that avoids risky actions near the cliff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f602cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with values of an optimal policy\n",
    "optimal_policy = np.zeros(48, dtype=int)\n",
    "for s in range(48):\n",
    "    if s < 36:\n",
    "        optimal_policy[s] = 1  \n",
    "    elif s >= 36 and s < 47:\n",
    "        optimal_policy[s] = 0 \n",
    "    if s % 12 == 11:\n",
    "        optimal_policy[s] = 2 \n",
    "print_policy(optimal_policy)\n",
    "policy = np.eye(4)[optimal_policy] # One-hot encoding to get [nS x nA] shape\n",
    "optimal_V = policy_evaluation(SimpleCliffEnv(), policy)\n",
    "print_value_function(optimal_V)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
